{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "validus_path = \"/Users/ramnarayanchoudhary/Documents/GitHub/validusBoxes\"\n",
    "sys.path.append(validus_path)\n",
    "original_cwd = os.getcwd()\n",
    "os.chdir(validus_path)\n",
    "\n",
    "\n",
    "from storage import STORAGE\n",
    "import boxes.Frame.docling\n",
    "import utils.unclassified\n",
    "import storage\n",
    "import importlib\n",
    "stuffToReImport=[storage,boxes.Frame.docling,utils.unclassified]\n",
    "# here \n",
    "for myModule in stuffToReImport:\n",
    "        importlib.reload(myModule)\n",
    "# from docling.document_converter import DocumentConverter\n",
    "# from utils.unclassified import getArrayOfStructFromDF\n",
    "from processor import processBox\n",
    "# import pandas as pd\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=['075-013480.pdf','075-897564.pdf','077-010300.pdf','075-897674.pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myStorageConfig={\n",
    "    'defaultFileStorage':'onPrem',\n",
    "}\n",
    "client='manualBrokerage'\n",
    "\n",
    "myStorage=STORAGE(client,myStorageConfig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesToSkip=[]\n",
    "for file in files:\n",
    "    \n",
    "\n",
    "    if file in filesToSkip:\n",
    "        continue\n",
    "    print(f\"Processing file: {file}\")\n",
    "    aSLABox={\n",
    "        'extraConfig':{\n",
    "            'fileName':file\n",
    "        },\n",
    "        'boxesPostProcess':[\n",
    "            {\n",
    "                \"UBI\":\"BOX::Frame.docling.dividePDFIntoPages\",\n",
    "                \"extraConfig\":{},\n",
    "                'boxesPostProcess':[\n",
    "                    {\n",
    "                        \"UBI\":\"BOX::Frame.docling.DoclingExtractByPage\",\n",
    "                        \"extraConfig\":{}\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    processBox(\"BOX::Frame.fileClassify.FileClassify\",client,aSLABox,{},'now',myStorage)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline: for every individual page → load its tables → normalise columns → clean rows → only then merge the already-clean pages.\n",
    "from __future__ import annotations\n",
    "import re, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional\n",
    "from storage import STORAGE\n",
    "\n",
    "class PDFHoldingsProcessor:\n",
    "    #  construction \n",
    "    def __init__(self, storage_config: Dict, client: str):\n",
    "        self.storage = STORAGE(client, storage_config)\n",
    "\n",
    "    #  generic helpers \n",
    "    @staticmethod\n",
    "    def _strip(x):                        \n",
    "        return x.strip() if isinstance(x, str) else x\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_numeric(v) -> Optional[float]:\n",
    "        if v is None or (isinstance(v, float) and np.isnan(v)):\n",
    "            return np.nan\n",
    "        if isinstance(v, (int, float)):\n",
    "            return float(v)\n",
    "\n",
    "        txt, neg = str(v).strip(), False\n",
    "        if txt.startswith('(') and txt.endswith(')'): neg, txt = True, txt[1:-1]\n",
    "        if txt.endswith('-'):                        neg, txt = True, txt[:-1]\n",
    "        txt = re.sub(r'[^0-9.\\-]', '', txt)                 # keep digits / dot / minus\n",
    "        if txt.count('.') > 1:                              # squash thousands-dots\n",
    "            a, b = txt.split('.', 1); txt = a + '.' + b.replace('.', '')\n",
    "        try:\n",
    "            num = float(txt);  num = -num if neg else num\n",
    "            return abs(num) if num < 0 and abs(num) < 1e12 else num\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "\n",
    "    #  column mapping \n",
    "    _RULES = {\n",
    "        'SHARES_UNITS'   : ['shares', 'units'],\n",
    "        'DESCRIPTION'    : ['description', 'security', 'name', 'fund'],\n",
    "        'BEGINNING_MARKET': ['beginning', 'start'],\n",
    "        'ENDING_MARKET'  : ['ending', 'market', 'final'],\n",
    "        'ADJUSTED_COST'  : ['adjusted', 'cost', 'basis'],\n",
    "        'GAIN_LOSS'      : ['gain', 'loss', 'unrealized'],\n",
    "    }\n",
    "    _CORE = list(_RULES.keys())\n",
    "\n",
    "    def _detect_schema(self, df: pd.DataFrame) -> Dict[str, str]:\n",
    "        out = {}\n",
    "        for col in df.columns:\n",
    "            norm = str(col).lower().replace(' ','')\n",
    "            for tgt, keys in self._RULES.items():\n",
    "                if any(k in norm for k in keys):\n",
    "                    out[col] = tgt; break\n",
    "        return out\n",
    "\n",
    "    def _standardise(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        mapping = self._detect_schema(df)\n",
    "        res = pd.DataFrame(index=df.index)\n",
    "        for tgt in self._CORE:\n",
    "            src = next((c for c,t in mapping.items() if t==tgt), None)\n",
    "            res[tgt] = df[src] if src else np.nan\n",
    "        return res\n",
    "\n",
    "    #  text helpers \n",
    "    def _page_text(self, pdf_id:str, page:str) -> List[str]:\n",
    "        f = Path(self.storage.getDir('l1',[pdf_id,'doclingByPage',page,'docling_texts.txt']))\n",
    "        return f.read_text(encoding='utf-8').splitlines() if f.exists() else []\n",
    "    \n",
    "    def _page_markdown(self, pdf_id:str, page:str) -> List[str]:\n",
    "        f = Path(self.storage.getDir('l1',[pdf_id,'doclingByPage',page,'docling_markdown.txt']))\n",
    "        return f.read_text(encoding='utf-8').splitlines() if f.exists() else []\n",
    "\n",
    "    def _is_holdings(self, pdf_id:str, page:str) -> bool:\n",
    "        lines = [l.strip().lower() for l in self._page_text(pdf_id,page)]\n",
    "        return lines.count('asset detail') == 1          \n",
    "    def _pages(self, pdf_id:str)->List[str]:\n",
    "        base = Path(self.storage.getDir('l1',[pdf_id,'doclingByPage']))\n",
    "        return sorted([p.name for p in base.iterdir() if p.is_dir() and p.name.isdigit()])\n",
    "    #  table loading \n",
    "    def _load_tables(self, pdf_id:str, page:str)->List[pd.DataFrame]:\n",
    "        d = Path(self.storage.getDir('l1',[pdf_id,'doclingByPage',page,'docling_tables']))\n",
    "        dfs=[]\n",
    "        for csv in sorted(d.glob('*.csv')):\n",
    "            rel = Path(pdf_id)/'doclingByPage'/page/'docling_tables'/csv.stem\n",
    "            df = self.storage.getFullTableAsDF('l1',str(rel))\n",
    "            if not df.empty: dfs.append(df)\n",
    "        return dfs\n",
    "    #  cleaner \n",
    "    def _clean(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        if df.empty: return df\n",
    "        c = df.applymap(self._strip).copy()\n",
    "\n",
    "        for n in ['SHARES_UNITS','BEGINNING_MARKET','ENDING_MARKET','ADJUSTED_COST','GAIN_LOSS']:\n",
    "            if n in c: c[n] = c[n].apply(self._parse_numeric)\n",
    "        if 'SHARES_UNITS' in c: c['SHARES_UNITS'] = c['SHARES_UNITS'].abs()\n",
    "\n",
    "        desc = c['DESCRIPTION'].fillna('')\n",
    "        header_re = re.compile(\n",
    "            r'^(TOTAL(?!\\s+RETURN)|SUBTOTAL|NET\\s+ASSETS|GAIN\\s*/\\s*LOSS|UNREALIZED|COLLECTIVE|PARTICIPANT|NOTES|CASH)$',\n",
    "            re.I\n",
    "        )\n",
    "        good = desc.str.len().gt(2) & ~desc.str.match(header_re)\n",
    "        nums = (c['SHARES_UNITS'].gt(0) | c['ENDING_MARKET'].gt(0))\n",
    "        c = c[good & nums]\n",
    "\n",
    "        def _ticker(t):\n",
    "            for p in [r'TICKER[:\\s]+([A-Z0-9.@]{2,12})',r'\\b([A-Z]{2,6}X?)\\b']:\n",
    "                m=re.search(p,t.upper()); \n",
    "                if m and m.group(1) not in {'FUND','INDEX','PRICE','MONTH','END','TICKER'}: return m.group(1)\n",
    "            return np.nan\n",
    "        def _fname(t):\n",
    "            t=re.sub(r'TICKER[:\\s]+[A-Z0-9.@]{2,12}','',t,flags=re.I)\n",
    "            t=re.sub(r'MONTH\\s+END\\s+PRICE\\s*\\d*\\.?\\d*','',t,flags=re.I)\n",
    "            return re.sub(r'\\s{2,}',' ',t).strip()\n",
    "\n",
    "        c['TICKER']   = c['DESCRIPTION'].apply(_ticker)\n",
    "        c['FUND_NAME']= c['DESCRIPTION'].apply(_fname)\n",
    "        #print(c[['FUND_NAME','TICKER']+self._CORE])\n",
    "        return c[['FUND_NAME','TICKER']+self._CORE]\n",
    "\n",
    "    # metadata \n",
    "    def _metadata(self, text:List[str],txt2:List[str])->Dict:\n",
    "        flat = ' '.join(text)\n",
    "        dates=[]\n",
    "        for m in re.finditer(r'\\b(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4})\\b',flat):\n",
    "            ds=m.group(1); sep='/' if '/' in ds else '-'\n",
    "            for f in [f'%m{sep}%d{sep}%Y',f'%m{sep}%d{sep}%y']:\n",
    "                try: dates.append(datetime.strptime(ds,f).strftime('%Y-%m-%d')); break\n",
    "                except: pass\n",
    "        # below is the heavy pdf specific logic here \n",
    "        try:\n",
    "            acc_i = next(i for i, l in enumerate(txt2) if 'ACCOUNT NUMBER' in l.upper())\n",
    "        except StopIteration:\n",
    "            return None\n",
    "        start_i = 0\n",
    "        for j in range(acc_i, -1, -1):\n",
    "            u = txt2[j].upper()\n",
    "            if 'BANK' in u or 'TRUST' in u or 'IMAGE' in u:\n",
    "                start_i = j + 1\n",
    "                break\n",
    "        window = [l.strip() for l in txt2[start_i:acc_i + 1] if l.strip()]\n",
    "        if not window:\n",
    "            return None\n",
    "        joined = ' '.join(window)\n",
    "        m = re.search(r'^(.*?)(?=\\s*ACCOUNT NUMBER)', joined, re.I)\n",
    "        plan_name = m.group(1).strip() if m else joined.strip()\n",
    "        print(plan_name)\n",
    "        print(\"this is plan name\")\n",
    "        parts = [p.strip() for p in re.split(r'[;,]', plan_name)]\n",
    "\n",
    "        print(parts)\n",
    "        return {'as_of_date': dates[-1] if dates else None,\n",
    "                'plan_name':parts[0],'counterparty_name': \"TRUST BANK\",'plan_number':parts[-1]}\n",
    "    #  store \n",
    "    def _save(self,pdf_id:str,payload:Dict):\n",
    "        out = Path(self.storage.getDir('l1',[pdf_id,'doclingByPage']))/'output.json'\n",
    "        out.write_text(json.dumps(payload,indent=2,default=str))\n",
    "\n",
    "    #  main API \n",
    "    def process_pdf(self, pdf_id:str):\n",
    "        pages=self._pages(pdf_id); all_text=[]; cleaned=[]\n",
    "        for p in pages:\n",
    "            all_text+=self._page_text(pdf_id,p)\n",
    "            txt2=self._page_markdown(pdf_id,p)\n",
    "            # print(\"this is markdown txt how this look like\", txt2)\n",
    "            if not self._is_holdings(pdf_id,p): continue\n",
    "\n",
    "            raw=pd.concat([self._standardise(t) for t in self._load_tables(pdf_id,p)],\n",
    "                          ignore_index=True) if self._load_tables(pdf_id,p) else pd.DataFrame()\n",
    "            cln=self._clean(raw); \n",
    "\n",
    "            print(\"now i want to print the cleaned dataframes\")\n",
    "            #print(cln)\n",
    "            if not cln.empty: cleaned.append(cln)\n",
    "\n",
    "        if not cleaned:\n",
    "            print(f' No holdings extracted for {pdf_id}'); return None\n",
    "\n",
    "        df=pd.concat(cleaned,ignore_index=True)\n",
    "\n",
    "        print(\"now i want to see the entire dataframes\")\n",
    "        #print(df)\n",
    "        data={'holdings':[{\n",
    "            'security_name':r.FUND_NAME,\n",
    "            'ticker': r.TICKER if pd.notna(r.TICKER) else None,\n",
    "            'cusip': None,\n",
    "            'units_held': float(r.SHARES_UNITS) if pd.notna(r.SHARES_UNITS) else None,\n",
    "            'last_price': round(r.ENDING_MARKET/r.SHARES_UNITS,6) if (pd.notna(r.ENDING_MARKET) and pd.notna(r.SHARES_UNITS) and r.SHARES_UNITS) else None,\n",
    "            'market_value': float(r.ENDING_MARKET) if pd.notna(r.ENDING_MARKET) else None,\n",
    "            'currency':'USD','accrued_interest':None\n",
    "        } for _,r in df.iterrows()], **self._metadata(all_text,txt2)}\n",
    "        self._save(pdf_id,data)\n",
    "        print(f'Processed {len(data[\"holdings\"])} holdings for {pdf_id}')\n",
    "        return data\n",
    "\n",
    "    def process_all_pdfs(self):\n",
    "        res={}; ids=self.storage.getAllLayerNFiles('l1','')\n",
    "        for i in ids:\n",
    "            try: res[i]=self.process_pdf(i)\n",
    "            except Exception as e:\n",
    "                print(f' Error {i}: {e}'); res[i]=None\n",
    "        return res\n",
    "\n",
    "#  run \n",
    "storage_config={'defaultFileStorage':'onPrem'}\n",
    "proc=PDFHoldingsProcessor(storage_config,'manualBrokerage')\n",
    "results=proc.process_all_pdfs()\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
